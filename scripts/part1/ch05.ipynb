{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *Support Vector Machine* (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. This is called *large margin classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instances on the margin lines are called support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we strictly impose that all instances must be off the street and on the right side, this is called *hard margin classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard margin classification is susceptible to outliers. To avoid these issues, use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called *soft margin classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM, mathematical technique called the *kernel trick* can be applied. The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much each instance resembles a particular *landmark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *radial basis function* (RBF) is a real-valued function whose value depends only on the distance between the input and some fixed point, either the origin, or some other fixed point called center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian RBF\n",
    "\n",
    "$\\phi_{\\gamma}(\\textbf{x}, l) = \\text{exp}(-\\gamma \\parallel \\textbf{x} - l \\parallel^{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other kernels exist but are used much more rarely. Some kernels are specialized for specific data structures. *String kernels* are sometimes used when classifying text documents or DNA sequences (e.g., using the *string subsequence kernel* or kernels based on the *Levenshtein distance*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ab instance has *sparse features* if it has few nonzero features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more training instances within the margin does not affect SVM regression predictions; thus, the model is said to be $\\epsilon$-*insensitive*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM classifier prediction\n",
    "\n",
    "$\\hat y = 0 \\quad \\text{if} \\quad \\textbf{w}^{T}\\textbf{x} + b < 0$\n",
    "\n",
    "Otherwise $\\hat y$ is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a large margin, we need to minimize $\\parallel \\textbf{w} \\parallel$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$t^{(i)} = -1$ if $y^{(i)} = 0$. Otherwise it is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard margin linear SVM classifier objective\n",
    "\n",
    "minimize $\\frac{1}{2}\\textbf{w}^{T}\\textbf{w}$\n",
    "\n",
    "subject to $t^{(i)}(\\textbf{w}^{T}\\textbf{x}^{(i)} + b ) \\geq 1$ for $i = 1, 2, \\cdots, m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the soft margin objective, we need to introduce a *slack variable* $\\zeta^{(i)} \\geq 0$ for each instance: $\\zeta^{(i)} \\geq 0$ measures how much the instance is allowed to violate the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft margin linear SVM classifier objective\n",
    "\n",
    "minimize $\\frac{1}{2}\\textbf{w}^{T}\\textbf{w} + C \\sum\\limits_{i = 1}^{m} \\zeta^{(i)}$\n",
    "\n",
    "subject to $t^{(i)}(\\textbf{w}^{T}\\textbf{x}^{(i)} + b ) \\geq 1 - \\zeta^{(i)}$ and $\\zeta^{(i)} \\geq 0 $ for $i = 1, 2, \\cdots, m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as *Quadratic Programming* (QP) problems (Look at pg 168 for further details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a constrained optimization problem, known as the *primal problem*, it is possible to express a different but closely related problem, called its *dual problem* (Look at pg 169 for further details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, a *kernel* is a function capable of computing the dot product of some transformation, which is a function of two different vectors, based only on the original vectors, without having to compute (or even to know about) the transformation (Look at pg 169 through 172 for further details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM classifier cost function\n",
    "\n",
    "$J(\\textbf{w}, b) =  \\frac{1}{2}\\textbf{w}^{T}\\textbf{w} + C \\sum\\limits_{i = 1}^{m} max(0, 1 - t^{(i)}(\\textbf{w}^{T}\\textbf{x}^{(i)} + b ))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $max(0, 1 â€“ t)$ is called the *hinge loss* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
